{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EASY 21\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Implement the game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Color(Enum):\n",
    "    RED = -1\n",
    "    BLACK = 1\n",
    "\n",
    "\n",
    "class Card:\n",
    "    def __init__(self, color: Color, value: int):\n",
    "        self.color = color\n",
    "        self.value_ = value\n",
    "\n",
    "    @property\n",
    "    def value(self) -> int:\n",
    "        return self.value_ * self.color.value\n",
    "\n",
    "    @staticmethod\n",
    "    def draw_card() -> \"Card\":\n",
    "        value = int(np.random.uniform(1, 11, None))\n",
    "        color = np.random.choice([Color.RED, Color.BLACK], p=[1 / 3, 2 / 3])\n",
    "\n",
    "        return Card(color, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(\n",
    "        self,\n",
    "        player_sum: int,\n",
    "        dealer_sum: int,\n",
    "        is_terminal=False,\n",
    "    ):\n",
    "        self.player_sum = player_sum\n",
    "        self.dealer_sum = dealer_sum\n",
    "        self.is_terminal = is_terminal\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"Player sum: {self.player_sum}, Dealer sum: {self.dealer_sum}\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)\n",
    "\n",
    "    def is_bust(self, value: int) -> bool:\n",
    "        return value > 21 or value < 1\n",
    "\n",
    "    def is_player_bust(self) -> bool:\n",
    "        return self.is_bust(self.player_sum)\n",
    "\n",
    "    def is_dealer_bust(self) -> bool:\n",
    "        return self.is_bust(self.dealer_sum)\n",
    "\n",
    "    @staticmethod\n",
    "    def initial_state() -> \"State\":\n",
    "        player_sum = int(np.random.uniform(1, 11, None))\n",
    "        dealer_card = int(np.random.uniform(1, 11, None))\n",
    "\n",
    "        return State(player_sum, dealer_card)\n",
    "\n",
    "\n",
    "class Action(Enum):\n",
    "    HIT = 0\n",
    "    STICK = 1\n",
    "\n",
    "    @staticmethod\n",
    "    def from_value(value) -> \"Action\":\n",
    "        if value == Action.HIT.value:\n",
    "            return Action.HIT\n",
    "        return Action.STICK\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    @staticmethod\n",
    "    def step(state: State, action: Action) -> tuple[State, int]:\n",
    "        if state.is_terminal:\n",
    "            raise ValueError(\"Cannot step in terminal state\")\n",
    "\n",
    "        state = State(state.player_sum, state.dealer_sum)\n",
    "\n",
    "        if action == Action.HIT:\n",
    "            card = Card.draw_card()\n",
    "            state.player_sum += card.value\n",
    "            if state.is_player_bust():\n",
    "                state.is_terminal = True\n",
    "                return state, -1\n",
    "            else:\n",
    "                return state, 0\n",
    "\n",
    "        elif action == Action.STICK:\n",
    "            state.is_terminal = True\n",
    "            while state.dealer_sum < 17:\n",
    "                card = Card.draw_card()\n",
    "                state.dealer_sum += card.value\n",
    "                if state.is_dealer_bust():\n",
    "                    return state, 1\n",
    "\n",
    "            if state.dealer_sum > state.player_sum:\n",
    "                return state, -1\n",
    "            elif state.dealer_sum < state.player_sum:\n",
    "                return state, 1\n",
    "            else:\n",
    "                return state, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class Agent(ABC):\n",
    "    MAX_DEALER_SUM = 10\n",
    "    MAX_PLAYER_SUM = 21\n",
    "    MAX_ACTIONS = 2\n",
    "\n",
    "    def __init__(self, discount_factor: float = 1) -> None:\n",
    "        self.action_value = Agent.initialize_vector()\n",
    "        self.N0 = 100\n",
    "        self.gamma = discount_factor\n",
    "\n",
    "    def epsilon_greedy_action(self, state: State, eps: float) -> Action:\n",
    "        if state.is_terminal:\n",
    "            raise ValueError(\"Cannot choose action in terminal state\")\n",
    "\n",
    "        if eps > np.random.random():\n",
    "            return np.random.choice(list(Action))\n",
    "\n",
    "        action = np.argmax(self.action_value[state.dealer_sum, state.player_sum])\n",
    "        return Action.from_value(action)\n",
    "\n",
    "    @property\n",
    "    def state_value(self):\n",
    "        state_value = self.initialize_vector()[..., 0]\n",
    "\n",
    "        for d_sum in range(1, Agent.MAX_DEALER_SUM + 1):\n",
    "            for p_sum in range(1, Agent.MAX_DEALER_SUM + 1):\n",
    "                state_value[d_sum][p_sum] = max(self.action_value[d_sum][p_sum])\n",
    "\n",
    "        return state_value\n",
    "\n",
    "    def eval(self, n=1000):\n",
    "        result = 0\n",
    "        wins = 0\n",
    "        for _ in trange(n):\n",
    "            s = State.initial_state()\n",
    "            while not s.is_terminal:\n",
    "                action = self.epsilon_greedy_action(s, 0)\n",
    "                s, r = Environment.step(s, action)\n",
    "            result += r\n",
    "            if r == 1:\n",
    "                wins += 1\n",
    "        return (wins / n, result / n)\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_vector() -> np.ndarray:\n",
    "        return np.zeros(\n",
    "            (\n",
    "                Agent.MAX_DEALER_SUM + 1,\n",
    "                Agent.MAX_PLAYER_SUM + 1,\n",
    "                Agent.MAX_ACTIONS,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, episodes: int):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Monte Carlo Control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloControl(Agent):\n",
    "    def __init__(self, discount_factor=1):\n",
    "        super().__init__(discount_factor)\n",
    "\n",
    "    def train(self, episodes: int):\n",
    "        counter = self.initialize_vector()\n",
    "\n",
    "        for _ in trange(episodes):\n",
    "            trajectory = []\n",
    "            s = State.initial_state()\n",
    "            while not s.is_terminal:\n",
    "                epsilon = self.N0 / (\n",
    "                    self.N0 + sum(counter[s.dealer_sum, s.player_sum, :])\n",
    "                )\n",
    "                a = self.epsilon_greedy_action(s, epsilon)\n",
    "                next_s, r = Environment.step(s, a)\n",
    "                trajectory.append((s, a, r))\n",
    "                s = next_s\n",
    "\n",
    "            returns = [trajectory[-1][-1]]\n",
    "            for i, (_, _, r) in enumerate(reversed(trajectory)):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                returns.append(r + self.gamma * returns[-1])\n",
    "            returns = list(reversed(returns))\n",
    "\n",
    "            for i, (s, a, _) in enumerate(trajectory):\n",
    "                state = [s.dealer_sum, s.player_sum]\n",
    "                counter[state[0], state[1], a.value] += 1\n",
    "                alpha = 1 / counter[state[0], state[1], a.value]\n",
    "                self.action_value[state[0], state[1], a.value] += alpha * (\n",
    "                    returns[i] - self.action_value[state[0], state[1], a.value]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [01:21<00:00, 12237.99it/s]\n",
      "100%|██████████| 100000/100000 [00:06<00:00, 14795.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'(wins %, avg reward): (0.52814, 0.0594)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = MonteCarloControl()\n",
    "agent.train(1_000_000)\n",
    "f\"(wins %, avg reward): {agent.eval(100_000)}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
